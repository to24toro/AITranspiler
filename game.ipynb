{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 14:31:09.417889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-14 14:31:10.362469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_keras as keras\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "from rl.network import ResNet\n",
    "from rl.mcts import MCTS\n",
    "from rl.buffer import ReplayBuffer, Sample\n",
    "from rl.game import Game, encode_state\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "base_path = \"graphs\"\n",
    "index = \"20241212\"\n",
    "qubits = config[\"game_settings\"][\"N\"]\n",
    "training_settings = config[\"training_settings\"]\n",
    "network_settings = config[\"network_settings\"]\n",
    "mcts_settings = config[\"mcts_settings\"]\n",
    "num_cpus = training_settings[\"num_cpus\"]\n",
    "num_gpus = training_settings[\"num_gpus\"]\n",
    "n_episodes = training_settings[\"n_episodes\"]\n",
    "buffer_size = training_settings[\"buffer_size\"]\n",
    "batch_size = training_settings[\"batch_size\"]\n",
    "epochs_per_update = training_settings[\"epochs_per_update\"]\n",
    "update_period = training_settings[\"update_period\"]\n",
    "save_period = training_settings[\"save_period\"]\n",
    "eval_period = training_settings.get(\"eval_period\", 100)\n",
    "\n",
    "\n",
    "def selfplay(weights, qubits, current_episode, config):\n",
    "    record = []\n",
    "    game = Game(qubits, config)\n",
    "    state = game.get_initial_state()\n",
    "    game.reset_used_columns()\n",
    "    network = ResNet(action_space=game.action_space, config=config)\n",
    "    network.predict(encode_state(state, qubits))\n",
    "    network.set_weights(weights)\n",
    "\n",
    "    mcts = MCTS(qubits=qubits, network=network, config=config)\n",
    "    done = False\n",
    "    total_score = 0\n",
    "    step_count = 0\n",
    "    prev_action = None\n",
    "\n",
    "    while not done and step_count < game.MAX_STEPS:\n",
    "        mcts_policy = mcts.search(\n",
    "            root_state=state,\n",
    "            prev_action=prev_action,\n",
    "            num_simulations=mcts_settings[\"num_mcts_simulations\"],\n",
    "        )\n",
    "\n",
    "        if prev_action is not None:\n",
    "            indices = [i for i in range(game.action_space) if i != prev_action]\n",
    "            valid_actions = game.get_valid_actions(state, prev_action)\n",
    "            prob = mcts_policy[valid_actions]\n",
    "            prob = prob / prob.sum()\n",
    "            action = np.random.choice(valid_actions, p=prob)\n",
    "        else:\n",
    "            indices = list(range(game.action_space))\n",
    "            prob = mcts_policy\n",
    "            action = np.random.choice(indices, p=prob)\n",
    "        record.append(Sample(state.copy(), mcts_policy, reward=None))\n",
    "        state, done, action_score = game.step(state, action, prev_action)\n",
    "        prev_action = action\n",
    "        # print(state, action_score, done)\n",
    "        total_score += action_score\n",
    "        step_count += 1\n",
    "\n",
    "    reward = game.get_reward(state, total_score)\n",
    "    for sample in record:\n",
    "        sample.reward = reward\n",
    "    return record\n",
    "\n",
    "\n",
    "def evaluate_self_play(qubits, network, config):\n",
    "    pattern = os.path.join(base_path, f\"adj_matrix_{qubits}_*.npy\")\n",
    "    file_paths = glob.glob(pattern)\n",
    "    avg_depth = []\n",
    "    avg_counts = []\n",
    "    for file_path in file_paths:\n",
    "        state = np.load(file_path)\n",
    "        game = Game(qubits, config)\n",
    "        swap_pairs = []\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        prev_action = None\n",
    "        while not done and step_count < game.MAX_STEPS:\n",
    "            encoded_state = encode_state(state, qubits)\n",
    "            input_state = np.expand_dims(encoded_state, axis=0)\n",
    "            policy_output, value_output = network.predict(input_state)\n",
    "            policy = np.array(policy_output)[0]\n",
    "            # policy = tf.nn.softmax(policy_logits).numpy()[0]\n",
    "            # valid_actions = game.get_valid_actions(state, prev_action)\n",
    "            if prev_action is not None:\n",
    "                indices = [i for i in range(game.action_space) if i != prev_action]\n",
    "                try:\n",
    "                    valid_actions = game.get_valid_actions(state, prev_action)\n",
    "                    prob = policy[valid_actions]\n",
    "                except:\n",
    "                    prob = policy[indices]\n",
    "                try:\n",
    "                    action = np.random.choice(valid_actions, p=prob / prob.sum())\n",
    "                except:\n",
    "                    action = np.random.choice(valid_actions)\n",
    "            else:\n",
    "                indices = list(range(game.action_space))\n",
    "                prob = policy\n",
    "                action = np.random.choice(indices, p=prob / prob.sum())\n",
    "            if action < len(game.coupling_map):\n",
    "                selected_action = game.coupling_map[action]\n",
    "                swap_pairs.append(selected_action)\n",
    "            else:\n",
    "                for pair in game.coupling_map[action%2::2]:\n",
    "                    swap_pairs.append(pair)\n",
    "            state, done, _ = game.step(state, action, prev_action)\n",
    "            prev_action = action\n",
    "            step_count += 1\n",
    "        if not done:\n",
    "            depth = game.MAX_STEPS\n",
    "            swap_count = game.MAX_STEPS\n",
    "        else:\n",
    "            game.current_layer+=1\n",
    "            depth = game.current_layer\n",
    "            swap_count = len(swap_pairs)\n",
    "        print(f\"depth: {depth}, count: {swap_count}\")\n",
    "        avg_counts.append(swap_count)\n",
    "        avg_depth.append(depth)\n",
    "    return avg_depth, avg_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 14:31:11.888423: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 14:31:11.991698: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 14:31:11.992497: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 14:31:11.997151: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 14:31:11.997669: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 14:31:11.998106: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 14:31:12.538261: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 14:31:12.538999: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 14:31:12.539014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-12-14 14:31:12.539460: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-12-14 14:31:12.539512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2013 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-12-14 14:31:14.348732: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8905\n",
      " 24%|██▍       | 12/50 [2:35:02<8:19:56, 789.39s/it]"
     ]
    }
   ],
   "source": [
    "logdir = Path(\"log\")\n",
    "if logdir.exists():\n",
    "    shutil.rmtree(logdir)\n",
    "summary_writer = tf.summary.create_file_writer(str(logdir))\n",
    "\n",
    "game = Game(qubits, config)\n",
    "network = ResNet(action_space=game.action_space, config=config)\n",
    "\n",
    "dummy_state = encode_state(game.state, qubits)\n",
    "network.predict(encode_state(game.state, qubits))\n",
    "current_weights = network.get_weights()\n",
    "\n",
    "optimizer = keras.optimizers.legacy.Adam(\n",
    "    learning_rate=network_settings[\"learning_rate\"]\n",
    ")\n",
    "\n",
    "replay = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "n_updates = 0\n",
    "\n",
    "n = 0\n",
    "while n < n_episodes:\n",
    "    for _ in tqdm(range(update_period)):\n",
    "        finished = selfplay(current_weights, qubits, n, config)\n",
    "        replay.add_record(finished)\n",
    "        n += 1\n",
    "    print(\"-\" * 50)\n",
    "    if len(replay) >= batch_size:\n",
    "        num_iters = epochs_per_update * (len(replay) // batch_size)\n",
    "        value_loss_weight = 0.5\n",
    "        policy_loss_weight = 1.5\n",
    "        for i in tqdm(range(num_iters)):\n",
    "            states, mcts_policy, rewards = replay.get_minibatch(batch_size=batch_size)\n",
    "            with tf.GradientTape() as tape:\n",
    "                p_pred, v_pred = network(states, training=True)\n",
    "                value_loss = tf.square(rewards - v_pred)\n",
    "                policy_loss = -tf.reduce_sum(\n",
    "                    mcts_policy * tf.math.log(p_pred + 1e-5), axis=1, keepdims=True\n",
    "                )\n",
    "                loss = tf.reduce_mean(\n",
    "                    value_loss_weight * value_loss + policy_loss_weight * policy_loss\n",
    "                )\n",
    "            grads = tape.gradient(loss, network.trainable_variables)\n",
    "            grads, _ = tf.clip_by_global_norm(grads, 0.5)\n",
    "            optimizer.apply_gradients(zip(grads, network.trainable_variables))\n",
    "            n_updates += 1\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar(\n",
    "                        \"value_loss\", tf.reduce_mean(value_loss), step=n_updates\n",
    "                    )\n",
    "                    tf.summary.scalar(\n",
    "                        \"policy_loss\", tf.reduce_mean(policy_loss), step=n_updates\n",
    "                    )\n",
    "\n",
    "        current_weights = network.get_weights()\n",
    "\n",
    "    if n % save_period == 0:\n",
    "        network.save(f\"checkpoints/network{qubits}_{index}_{n}\", save_format=\"tf\")\n",
    "        network.save_weights(f\"checkpoints/network{qubits}_{index}_{n}.weights.h5\")\n",
    "        print(\"-\" * 50)\n",
    "    if n % eval_period == 0:\n",
    "        depth, count = evaluate_self_play(qubits, network, config)\n",
    "        print(\n",
    "            f\"Episode {n}: SWAP depth is {np.mean(depth)}, SWAP count is {np.mean(count)}\"\n",
    "        )\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game(qubits, config)\n",
    "network = ResNet(action_space=game.action_space, config=config)\n",
    "network = keras.models.load_model(f\"checkpoints/network{qubits}_{index}_500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "for _ in range(20):\n",
    "    depth, count = evaluate_self_play(qubits, network, config)\n",
    "    depths.append(depth)\n",
    "min_depth = np.min(np.vstack(depths), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    game = Game(qubits, config)\n",
    "    state = game.state\n",
    "    ans = []\n",
    "    done = False\n",
    "    total_score = 0\n",
    "    step_count = 0\n",
    "    prev_action = None\n",
    "    print(state)\n",
    "    while not done and step_count < game.MAX_STEPS:\n",
    "        encoded_state = encode_state(state, qubits)\n",
    "        input_state = np.expand_dims(encoded_state, axis=0)\n",
    "\n",
    "        policy_output, value_output = network.predict(input_state)\n",
    "        policy = policy_output[0]\n",
    "        if prev_action is not None:\n",
    "            indices = [i for i in range(game.action_space) if i != prev_action]\n",
    "            prob = policy[indices]\n",
    "            action = np.random.choice(indices, p=prob / prob.sum())\n",
    "        else:\n",
    "            indices = list(range(game.action_space))\n",
    "            action = np.random.choice(indices, p=policy)\n",
    "        selected_action = game.coupling_map[action]\n",
    "        ans.append(selected_action)\n",
    "        state, done, _ = game.step(state, action, prev_action)\n",
    "        prev_action = action\n",
    "        step_count += 1\n",
    "    if done:\n",
    "        print(f\"Game finished successfully in {step_count} steps with {ans}\")\n",
    "    else:\n",
    "        print(f\"Game terminated after reaching the maximum steps ({game.MAX_STEPS}).\")\n",
    "        print(f\"Total score: {total_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(min_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array([1, 5, 6, 4, 6, 7, 7, 2, 6, 3, 2, 5, 3, 7, 5, 7, 6, 3, 5, 2, 4, 9,\n",
    "       2, 6, 5, 5, 3, 2, 2, 1]) -> 4.366666666666666"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_transpiler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
