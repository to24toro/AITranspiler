{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tf_keras as keras\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "from rl.network import ResNet\n",
    "from rl.mcts import MCTS\n",
    "from rl.buffer import ReplayBuffer, Sample\n",
    "from rl.game import Game, encode_state\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "base_path = \"graphs\"\n",
    "index = \"20241212\"\n",
    "qubits = config[\"game_settings\"][\"N\"]\n",
    "training_settings = config[\"training_settings\"]\n",
    "network_settings = config[\"network_settings\"]\n",
    "mcts_settings = config[\"mcts_settings\"]\n",
    "num_cpus = training_settings[\"num_cpus\"]\n",
    "num_gpus = training_settings[\"num_gpus\"]\n",
    "n_episodes = training_settings[\"n_episodes\"]\n",
    "buffer_size = training_settings[\"buffer_size\"]\n",
    "batch_size = training_settings[\"batch_size\"]\n",
    "epochs_per_update = training_settings[\"epochs_per_update\"]\n",
    "update_period = training_settings[\"update_period\"]\n",
    "save_period = training_settings[\"save_period\"]\n",
    "eval_period = training_settings.get(\"eval_period\", 100)\n",
    "\n",
    "\n",
    "def selfplay(weights, qubits, current_episode, config):\n",
    "    record = []\n",
    "    game = Game(qubits, config)\n",
    "    state = game.get_initial_state()\n",
    "    game.reset_used_columns()\n",
    "    network = ResNet(action_space=game.action_space, config=config)\n",
    "    network.predict(encode_state(state, qubits))\n",
    "    network.set_weights(weights)\n",
    "\n",
    "    mcts = MCTS(qubits=qubits, network=network, config=config)\n",
    "    done = False\n",
    "    total_score = 0\n",
    "    step_count = 0\n",
    "    prev_action = None\n",
    "\n",
    "    while not done and step_count < game.MAX_STEPS:\n",
    "        mcts_policy = mcts.search(\n",
    "            root_state=state,\n",
    "            prev_action=prev_action,\n",
    "            num_simulations=mcts_settings[\"num_mcts_simulations\"],\n",
    "        )\n",
    "        if prev_action is not None:\n",
    "            indices = [i for i in range(game.action_space) if i != prev_action]\n",
    "            valid_actions = game.get_valid_actions(state, prev_action)\n",
    "            prob = mcts_policy[valid_actions]\n",
    "            prob = prob / prob.sum()\n",
    "            action = np.random.choice(valid_actions, p=prob)\n",
    "        else:\n",
    "            indices = list(range(game.action_space))\n",
    "            prob = mcts_policy\n",
    "            action = np.random.choice(indices, p=prob)\n",
    "        record.append(Sample(state.copy(), mcts_policy, reward=None))\n",
    "        state, done, action_score = game.step(state, action, prev_action)\n",
    "        prev_action = action\n",
    "        # print(state, action_score, done)\n",
    "        total_score += action_score\n",
    "        step_count += 1\n",
    "\n",
    "    reward = game.get_reward(state, total_score)\n",
    "    for sample in record:\n",
    "        sample.reward = reward\n",
    "    return record\n",
    "\n",
    "\n",
    "def evaluate_self_play(qubits, network, config):\n",
    "    pattern = os.path.join(base_path, f\"adj_matrix_{qubits}_*.npy\")\n",
    "    file_paths = glob.glob(pattern)\n",
    "    avg_depth = []\n",
    "    avg_counts = []\n",
    "    for file_path in file_paths:\n",
    "        state = np.load(file_path)\n",
    "        # print(state)\n",
    "        game = Game(qubits, config)\n",
    "        swap_pairs = []\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        prev_action = None\n",
    "        while not done and step_count < game.MAX_STEPS:\n",
    "            encoded_state = encode_state(state, qubits)\n",
    "            input_state = np.expand_dims(encoded_state, axis=0)\n",
    "            policy_output, value_output = network.predict(input_state, verbose=0)\n",
    "            policy = np.array(policy_output)[0]\n",
    "            # policy = tf.nn.softmax(policy_logits).numpy()[0]\n",
    "            # valid_actions = game.get_valid_actions(state, prev_action)\n",
    "            if prev_action is not None:\n",
    "                indices = [i for i in range(game.action_space) if i != prev_action]\n",
    "                try:\n",
    "                    valid_actions = game.get_valid_actions(state, prev_action)\n",
    "                    prob = policy[valid_actions]\n",
    "                except:\n",
    "                    prob = policy[indices]\n",
    "                action = np.random.choice(valid_actions, p=prob / prob.sum())\n",
    "            else:\n",
    "                indices = list(range(game.action_space))\n",
    "                prob = policy\n",
    "                action = np.random.choice(indices, p=prob / prob.sum())\n",
    "            if action < len(game.coupling_map):\n",
    "                selected_action = game.coupling_map[action]\n",
    "                swap_pairs.append(selected_action)\n",
    "            else:\n",
    "                for pair in game.coupling_map[action%2::2]:\n",
    "                    swap_pairs.append(pair)\n",
    "            state, done, _ = game.step(state, action, prev_action)\n",
    "            prev_action = action\n",
    "            step_count += 1\n",
    "        if not done:\n",
    "            depth = game.MAX_STEPS\n",
    "            swap_count = game.MAX_STEPS\n",
    "        else:\n",
    "            game.current_layer+=1\n",
    "            depth = game.current_layer\n",
    "            swap_count = len(swap_pairs)\n",
    "        print(f\"depth: {depth}, count: {swap_count},swap:{swap_pairs}\")\n",
    "        avg_counts.append(swap_count)\n",
    "        avg_depth.append(depth)\n",
    "    return avg_depth, avg_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game(qubits, config)\n",
    "network = ResNet(action_space=game.action_space, config=config)\n",
    "network = keras.models.load_model(f\"checkpoints/network{qubits}_{index}_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_state_depth_like_sabre(qubits, network,config, state, reps=1):\n",
    "    min_depth = float(\"inf\")\n",
    "    res_count = 0\n",
    "    for _ in range(reps):\n",
    "        depth, count, swap_pairs = evaluate_self_play_like_sabre(qubits, network, config,state)\n",
    "        for col1,col2 in swap_pairs:\n",
    "            state[:, [col1, col2]] = state[:, [col2, col1]]\n",
    "            state[[col1, col2], :] = state[[col2, col1], :]\n",
    "        if depth < min_depth:\n",
    "            min_depth = depth\n",
    "            res_count = count\n",
    "    return min_depth, res_count, swap_pairs\n",
    "\n",
    "def evaluate_self_play_like_sabre(qubits, network, config,state):\n",
    "    game = Game(qubits, config)\n",
    "    swap_pairs = []\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    prev_action = None\n",
    "    while not done and step_count < game.MAX_STEPS:\n",
    "        encoded_state = encode_state(state, qubits)\n",
    "        input_state = np.expand_dims(encoded_state, axis=0)\n",
    "        policy_output, value_output = network.predict(input_state)\n",
    "        policy = np.array(policy_output)[0]\n",
    "        # policy = tf.nn.softmax(policy_logits).numpy()[0]\n",
    "        # valid_actions = game.get_valid_actions(state, prev_action)\n",
    "        if prev_action is not None:\n",
    "            indices = [i for i in range(game.action_space) if i != prev_action]\n",
    "            try:\n",
    "                valid_actions = game.get_valid_actions(state, prev_action)\n",
    "                prob = policy[valid_actions]\n",
    "            except:\n",
    "                prob = policy[indices]\n",
    "            action = np.random.choice(valid_actions, p=prob / prob.sum())\n",
    "        else:\n",
    "            indices = list(range(game.action_space))\n",
    "            prob = policy\n",
    "            action = np.random.choice(indices, p=prob / prob.sum())\n",
    "        if action < len(game.coupling_map):\n",
    "            selected_action = game.coupling_map[action]\n",
    "            swap_pairs.append(selected_action)\n",
    "        else:\n",
    "            for pair in game.coupling_map[action%2::2]:\n",
    "                swap_pairs.append(pair)\n",
    "        state, done, _ = game.step(state, action, prev_action)\n",
    "        prev_action = action\n",
    "        step_count += 1\n",
    "    if not done:\n",
    "        depth = game.MAX_STEPS\n",
    "        swap_count = game.MAX_STEPS\n",
    "    else:\n",
    "        game.current_layer+=1\n",
    "        depth = game.current_layer\n",
    "        swap_count = len(swap_pairs)\n",
    "    print(f\"depth: {depth}, count: {swap_count}\")\n",
    "    return depth, swap_count, swap_pairs\n",
    "\n",
    "pattern = os.path.join(base_path, f\"adj_matrix_{qubits}_*.npy\")\n",
    "file_paths = glob.glob(pattern)\n",
    "depths = []\n",
    "counts = []\n",
    "for file_path in tqdm(file_paths):\n",
    "    state = np.load(file_path)\n",
    "    min_depth, count,swap_pairs = evaluate_state_depth_like_sabre(qubits,network,config,state,reps=30)\n",
    "    depths.append(min_depth)\n",
    "    counts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(depths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = []\n",
    "for _ in range(40):\n",
    "    print(\"==========================\")\n",
    "    depth, count = evaluate_self_play(qubits, network, config)\n",
    "    depths.append(depth)\n",
    "min_depth = np.min(np.vstack(depths), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(min_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array([1, 5, 6, 4, 6, 7, 7, 2, 6, 3, 2, 5, 3, 7, 5, 7, 6, 3, 5, 2, 4, 9,\n",
    "       2, 6, 5, 5, 3, 2, 2, 1]) -> 4.366666666666666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#7 100 3.1666  24.366666"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
