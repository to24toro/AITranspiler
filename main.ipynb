{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-16 00:08:45.171921: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-16 00:08:46.216733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "\n",
    "from rl.network import ResNet\n",
    "from rl.mcts import MCTS\n",
    "from rl.buffer import ReplayBuffer, Sample\n",
    "from rl.game import Game\n",
    "\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "base_path = \"graphs\"\n",
    "index = \"20241212\"\n",
    "qubits = config[\"game_settings\"][\"N\"]\n",
    "training_settings = config[\"training_settings\"]\n",
    "network_settings = config[\"network_settings\"]\n",
    "mcts_settings = config[\"mcts_settings\"]\n",
    "num_cpus = training_settings[\"num_cpus\"]\n",
    "num_gpus = training_settings[\"num_gpus\"]\n",
    "n_episodes = training_settings[\"n_episodes\"]\n",
    "buffer_size = training_settings[\"buffer_size\"]\n",
    "batch_size = training_settings[\"batch_size\"]\n",
    "epochs_per_update = training_settings[\"epochs_per_update\"]\n",
    "update_period = training_settings[\"update_period\"]\n",
    "save_period = training_settings[\"save_period\"]\n",
    "eval_period = training_settings[\"eval_period\"]\n",
    "\n",
    "\n",
    "def selfplay(qubits, network, config, device=\"cpu\"):\n",
    "    record = []\n",
    "    game = Game(qubits, config)\n",
    "    state = game.get_initial_state()\n",
    "    game.reset_used_columns()\n",
    "\n",
    "    mcts = MCTS(qubits=qubits, network=network, config=config)\n",
    "    done = False\n",
    "    total_score = 0\n",
    "    step_count = 0\n",
    "    prev_action = None\n",
    "\n",
    "    while not done and step_count < game.MAX_STEPS:\n",
    "        mcts_policy = mcts.search(\n",
    "            root_state=state,\n",
    "            prev_action=prev_action,\n",
    "            num_simulations=mcts_settings[\"num_mcts_simulations\"],\n",
    "        )\n",
    "\n",
    "        if prev_action is not None:\n",
    "            indices = [i for i in range(game.action_space) if i != prev_action]\n",
    "            valid_actions = game.get_valid_actions(state, prev_action)\n",
    "            prob = mcts_policy[valid_actions]\n",
    "            prob = prob / prob.sum()\n",
    "            action = np.random.choice(valid_actions, p=prob)\n",
    "        else:\n",
    "            indices = list(range(game.action_space))\n",
    "            prob = mcts_policy\n",
    "            action = np.random.choice(indices, p=prob)\n",
    "        record.append(Sample(state.copy(), mcts_policy, reward=None))\n",
    "        state, done, action_score = game.step(state, action, prev_action)\n",
    "        prev_action = action\n",
    "        total_score += action_score\n",
    "        step_count += 1\n",
    "\n",
    "    reward = game.get_reward(state, total_score)\n",
    "    for sample in record:\n",
    "        sample.reward = reward\n",
    "    return record\n",
    "\n",
    "\n",
    "def evaluate_self_play(qubits, network, config, device=\"cpu\"):\n",
    "    pattern = os.path.join(base_path, f\"adj_matrix_{qubits}_*.npy\")\n",
    "    file_paths = glob.glob(pattern)\n",
    "    avg_depth = []\n",
    "    avg_counts = []\n",
    "    for file_path in file_paths:\n",
    "        state = np.load(file_path)\n",
    "        game = Game(qubits, config)\n",
    "        swap_pairs = []\n",
    "        done = False\n",
    "        step_count = 0\n",
    "        prev_action = None\n",
    "        while not done and step_count < game.MAX_STEPS:\n",
    "            network.eval()\n",
    "            with torch.no_grad():\n",
    "                policy_output, value_output = network(\n",
    "                    torch.tensor(state, dtype=torch.float32)\n",
    "                    .unsqueeze(0)\n",
    "                    .unsqueeze(0)\n",
    "                    .to(device)\n",
    "                )\n",
    "                policy = policy_output.cpu().numpy()[0]\n",
    "            if prev_action is not None:\n",
    "                indices = [i for i in range(game.action_space) if i != prev_action]\n",
    "                try:\n",
    "                    valid_actions = game.get_valid_actions(state, prev_action)\n",
    "                    prob = policy[valid_actions]\n",
    "                except:\n",
    "                    prob = policy[indices]\n",
    "                try:\n",
    "                    action = np.random.choice(valid_actions, p=prob / prob.sum())\n",
    "                except:\n",
    "                    action = np.random.choice(valid_actions)\n",
    "            else:\n",
    "                indices = list(range(game.action_space))\n",
    "                prob = policy\n",
    "                action = np.random.choice(indices, p=prob / prob.sum())\n",
    "            if action < len(game.coupling_map):\n",
    "                selected_action = game.coupling_map[action]\n",
    "                swap_pairs.append(selected_action)\n",
    "            else:\n",
    "                for pair in game.coupling_map[action % 2 :: 2]:\n",
    "                    swap_pairs.append(pair)\n",
    "            state, done, _ = game.step(state, action, prev_action)\n",
    "            prev_action = action\n",
    "            step_count += 1\n",
    "        if not done:\n",
    "            depth = game.MAX_STEPS\n",
    "            swap_count = game.MAX_STEPS\n",
    "        else:\n",
    "            game.current_layer += 1\n",
    "            depth = game.current_layer\n",
    "            swap_count = len(swap_pairs)\n",
    "        print(f\"depth: {depth}, count: {swap_count}\")\n",
    "        avg_counts.append(swap_count)\n",
    "        avg_depth.append(depth)\n",
    "    return avg_depth, avg_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_CUDA___slow_conv2d_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(update_period)):\n\u001b[1;32m     25\u001b[0m     network\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 26\u001b[0m     finished \u001b[38;5;241m=\u001b[39m \u001b[43mselfplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqubits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     replay\u001b[38;5;241m.\u001b[39madd_record(finished)\n\u001b[1;32m     28\u001b[0m     n \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 53\u001b[0m, in \u001b[0;36mselfplay\u001b[0;34m(qubits, network, config, device)\u001b[0m\n\u001b[1;32m     50\u001b[0m prev_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m step_count \u001b[38;5;241m<\u001b[39m game\u001b[38;5;241m.\u001b[39mMAX_STEPS:\n\u001b[0;32m---> 53\u001b[0m     mcts_policy \u001b[38;5;241m=\u001b[39m \u001b[43mmcts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprev_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_action\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmcts_settings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_mcts_simulations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prev_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m         indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(game\u001b[38;5;241m.\u001b[39maction_space) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m prev_action]\n",
      "File \u001b[0;32m~/AITranspiler/rl/mcts.py:70\u001b[0m, in \u001b[0;36mMCTS.search\u001b[0;34m(self, root_state, num_simulations, prev_action)\u001b[0m\n\u001b[1;32m     67\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_to_str(root_state)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP:\n\u001b[0;32m---> 70\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprev_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m valid_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39mget_valid_actions(root_state, prev_action)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valid_actions) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# Add Dirichlet noise to the policy for exploration.\u001b[39;00m\n",
      "File \u001b[0;32m~/AITranspiler/rl/mcts.py:139\u001b[0m, in \u001b[0;36mMCTS._expand\u001b[0;34m(self, state, prev_action)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03mExpand a state node in the tree by initializing its attributes.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m:return: The neural network's value prediction for the state.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_to_str(state)\n\u001b[0;32m--> 139\u001b[0m nn_policy, nn_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m nn_policy \u001b[38;5;241m=\u001b[39m nn_policy\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    142\u001b[0m nn_value \u001b[38;5;241m=\u001b[39m nn_value\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/AITranspiler/rl/network.py:127\u001b[0m, in \u001b[0;36mResNet.predict\u001b[0;34m(self, mat)\u001b[0m\n\u001b[1;32m    125\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(mat, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 127\u001b[0m     policy, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m policy, value\n",
      "File \u001b[0;32m~/ait/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ait/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/AITranspiler/rl/network.py:93\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;66;03m# Initial layers\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# Residual layers\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_blocks:\n",
      "File \u001b[0;32m~/ait/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ait/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/ait/lib/python3.10/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ait/lib/python3.10/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_CUDA___slow_conv2d_forward)"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logdir = Path(\"log\")\n",
    "if logdir.exists():\n",
    "    shutil.rmtree(logdir)\n",
    "summary_writer = SummaryWriter(log_dir=logdir)\n",
    "\n",
    "game = Game(qubits, config)\n",
    "network = ResNet(action_space=game.action_space, config=config).to(\"cpu\")\n",
    "\n",
    "dummy_input = (\n",
    "    torch.tensor(game.state, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(\"cpu\")\n",
    ")\n",
    "network(dummy_input)\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=network_settings[\"learning_rate\"])\n",
    "\n",
    "\n",
    "replay = ReplayBuffer(buffer_size=buffer_size)\n",
    "\n",
    "n_updates = 0\n",
    "\n",
    "n = 0\n",
    "while n < n_episodes:\n",
    "    for _ in tqdm(range(update_period)):\n",
    "        network.eval()\n",
    "        finished = selfplay(qubits, network, config)\n",
    "        replay.add_record(finished)\n",
    "        n += 1\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "    network.to(device)\n",
    "    if len(replay) >= batch_size:\n",
    "        num_iters = epochs_per_update * (len(replay) // batch_size)\n",
    "        value_loss_weight = 0.5\n",
    "        policy_loss_weight = 1.5\n",
    "\n",
    "        for i in tqdm(range(num_iters)):\n",
    "            states, mcts_policy, rewards = replay.get_minibatch(batch_size=batch_size)\n",
    "            states = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "            mcts_policy = torch.tensor(mcts_policy, dtype=torch.float32).to(device)\n",
    "            rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "            network.train()\n",
    "\n",
    "            policy_pred, value_pred = network(states)\n",
    "            value_loss = torch.mean((rewards - value_pred.squeeze()) ** 2)\n",
    "            policy_loss = -torch.sum(\n",
    "                mcts_policy * torch.log(policy_pred + 1e-5), dim=1\n",
    "            ).mean()\n",
    "            loss = value_loss_weight * value_loss + policy_loss_weight * policy_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(network.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "\n",
    "            n_updates += 1\n",
    "\n",
    "            if i % 5 == 0:\n",
    "                summary_writer.add_scalar(\"value_loss\", value_loss.item(), n_updates)\n",
    "                summary_writer.add_scalar(\"policy_loss\", policy_loss.item(), n_updates)\n",
    "\n",
    "    if n % save_period == 0:\n",
    "        torch.save(network.state_dict(), f\"checkpoints/network{qubits}_{index}_{n}.pth\")\n",
    "        print(f\"Model saved: checkpoints/network{qubits}_{index}_{n}.pth\")\n",
    "        print(\"-\" * 50)\n",
    "    if n % eval_period == 0:\n",
    "        network.eval()\n",
    "        with torch.no_grad():\n",
    "            depth, count = evaluate_self_play(qubits, network, config, device=device)\n",
    "        print(\n",
    "            f\"Episode {n}: SWAP depth is {np.mean(depth)}, SWAP count is {np.mean(count)}\"\n",
    "        )\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wn/ysh4jryd4w1_xxwktjdxk0jr0000gn/T/ipykernel_77355/3104328494.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  network.load_state_dict(torch.load(checkpoint_path))\n"
     ]
    }
   ],
   "source": [
    "game = Game(qubits, config)\n",
    "\n",
    "checkpoint_path = f\"checkpoints/network{qubits}_{index}_5.pth\"\n",
    "network = ResNet(action_space=game.action_space, config=config)\n",
    "network.load_state_dict(torch.load(checkpoint_path))\n",
    "network.eval()\n",
    "\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 8, count: 12\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 9, count: 11\n",
      "depth: 7, count: 11\n",
      "depth: 13, count: 13\n",
      "depth: 8, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 7, count: 9\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 8, count: 9\n",
      "depth: 7, count: 12\n",
      "depth: 13, count: 13\n",
      "depth: 9, count: 11\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 9, count: 12\n",
      "depth: 13, count: 13\n",
      "depth: 10, count: 11\n",
      "depth: 13, count: 13\n",
      "depth: 6, count: 9\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 6, count: 7\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 7, count: 10\n",
      "depth: 13, count: 13\n",
      "depth: 11, count: 13\n",
      "depth: 7, count: 12\n",
      "depth: 9, count: 12\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 8, count: 11\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 8, count: 12\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 7, count: 11\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 9, count: 10\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 8, count: 12\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 9, count: 12\n",
      "depth: 13, count: 13\n",
      "depth: 7, count: 11\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 7, count: 9\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 7, count: 10\n",
      "depth: 8, count: 13\n",
      "depth: 8, count: 9\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 9, count: 11\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 6, count: 9\n",
      "depth: 9, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 10, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 7, count: 10\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 10, count: 12\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 9, count: 12\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 13, count: 13\n",
      "depth: 9, count: 12\n",
      "depth: 13, count: 13\n",
      "depth: 9, count: 12\n",
      "depth: 13, count: 13\n"
     ]
    }
   ],
   "source": [
    "depths = []\n",
    "for _ in range(4):\n",
    "    depth, count = evaluate_self_play(qubits, network, config)\n",
    "    depths.append(depth)\n",
    "min_depth = np.min(np.vstack(depths), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13,  6,  8, 13,  7,  8, 10,  7,  9,  7,  7,  8,  8,  7,  7, 13,  7,\n",
       "        8,  7,  9,  9,  7, 13, 13,  9, 13,  9,  9,  6, 13])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model saved to checkpoints/network6_20241212_5.onnx\n"
     ]
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 1, qubits, qubits)  # 例: 入力が8x8の行列の場合\n",
    "onnx_path = f\"checkpoints/network{qubits}_{index}_5.onnx\"\n",
    "\n",
    "# モデルをONNX形式でエクスポート\n",
    "torch.onnx.export(\n",
    "    network,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"policy\", \"value\"],\n",
    "    dynamic_axes={\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"policy\": {0: \"batch_size\"},\n",
    "        \"value\": {0: \"batch_size\"},\n",
    "    },\n",
    "    opset_version=15,\n",
    ")\n",
    "\n",
    "print(f\"ONNX model saved to {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array([1, 5, 6, 4, 6, 7, 7, 2, 6, 3, 2, 5, 3, 7, 5, 7, 6, 3, 5, 2, 4, 9,\n",
    "       2, 6, 5, 5, 3, 2, 2, 1]) -> 4.366666666666666"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ait",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
